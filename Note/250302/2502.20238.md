# Summary of 2502.20238.pdf

# FINEREASON: A Benchmark for Evaluating Reasoning Capabilities of Large Language Models

Bài báo này giới thiệu FINEREASON, một bộ tiêu chuẩn được thiết kế để đánh giá và nâng cao khả năng suy luận của các mô hình ngôn ngữ lớn (LLMs) thông qua các câu đố logic. Nó nhấn mạnh tầm quan trọng của một quá trình suy luận có chủ đích và nhiều bước, đối lập với các phản ứng nhanh "Hệ thống 1" và giải quyết vấn đề phản ánh "Hệ thống 2". Các bộ tiêu chuẩn hiện tại chủ yếu tập trung vào độ chính xác của câu trả lời cuối cùng, bỏ qua việc kiểm tra các bước suy luận trung gian, điều này rất quan trọng để đánh giá khả năng phản ánh và sửa chữa sai lầm của một mô hình.

## Đóng góp của bài báo

FINEREASON bao gồm bốn loại câu đố: Sudoku, Tô màu đồ thị, Trò chơi 24, và Câu đố lưới, mỗi loại cho phép đánh giá chi tiết khả năng suy luận thông qua hai nhiệm vụ: kiểm tra trạng thái và chuyển đổi trạng thái. Những nhiệm vụ này đánh giá cách mà các mô hình đánh giá tình huống hiện tại và lập kế hoạch cho các bước tiếp theo, cung cấp cái nhìn sâu sắc về quy trình suy luận của chúng.

## Kết quả và phương pháp

Nghiên cứu cho thấy rằng các mô hình được đào tạo trên FINEREASON thể hiện hiệu suất cải thiện trong các nhiệm vụ suy luận toán học, với mức tăng lên đến 5.1% trên tập dữ liệu GSM8K. Các phát hiện tiết lộ những hạn chế đáng kể trong khả năng suy luận sâu của các mô hình đa năng, nhấn mạnh sự cần thiết của các phương pháp đánh giá toàn diện hơn, xem xét cả phản ánh và sửa chữa.

Tóm lại, FINEREASON là một công cụ quý giá để nâng cao hiểu biết và khả năng của các LLM trong các nhiệm vụ yêu cầu suy luận cao, gợi ý rằng việc đào tạo trên dữ liệu dựa trên câu đố có thể cải thiện các kỹ năng suy luận toán học rộng hơn.
