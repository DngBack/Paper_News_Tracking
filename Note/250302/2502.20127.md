# Summary of 2502.20127.pdf

# Subtask-oriented Reinforced Fine-Tuning (SoRFT)

Bài báo này giới thiệu về phương pháp **Subtask-oriented Reinforced Fine-Tuning (SoRFT)**, một cách tiếp cận mới nhằm nâng cao khả năng giải quyết vấn đề của các mô hình ngôn ngữ lớn (LLMs). Các khung truyền thống thường phụ thuộc vào các mô hình thương mại tốn kém và gặp khó khăn trong việc tổng quát hóa. SoRFT giải quyết những vấn đề này bằng cách phân chia vấn đề thành các tiểu nhiệm có cấu trúc: xác định tệp, xác định chức năng, xác định dòng và tạo mã chỉnh sửa.

## Đóng góp của bài báo

Quá trình đào tạo của SoRFT bao gồm hai giai đoạn: 
1. **Fine-tuning có giám sát với mẫu từ chối**: giai đoạn này lọc dữ liệu Chain-of-Thought (CoT) bằng cách sử dụng các câu trả lời đúng.
2. **Học tăng cường dựa trên quy tắc**: sử dụng Proximal Policy Optimization (PPO) với phần thưởng từ các câu trả lời đúng.

## Kết quả và phương pháp

Các mô hình được đào tạo bằng SoRFT thể hiện hiệu suất hàng đầu trên các chuẩn như **SWE-Bench Verified** và **SWE-Bench Lite**, vượt trội hơn so với các mô hình mã nguồn mở hiện có. Kết quả cho thấy SoRFT cải thiện đáng kể khả năng giải quyết vấn đề và khả năng tổng quát, đồng thời cung cấp một lựa chọn tiết kiệm chi phí hơn so với các mô hình thương mại. Bài báo cũng thảo luận về tầm quan trọng của các quy tắc phần thưởng vững chắc trong học tăng cường và nhấn mạnh tiềm năng của SoRFT trong việc nâng cao khả năng của LLMs trong nhiều nhiệm vụ lập trình khác ngoài việc giải quyết vấn đề.
