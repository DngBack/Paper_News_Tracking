# Summary of 2502.20082.pdf

# LongRoPE2: Enhancing Context Windows in Large Language Models

Bài viết này giới thiệu LongRoPE2, một phương pháp mới được phát triển nhằm cải thiện kích thước ngữ cảnh hiệu quả của các mô hình ngôn ngữ lớn (LLMs) mà vẫn duy trì hiệu suất trên các ngữ cảnh ngắn. LongRoPE2 giải quyết các vấn đề liên quan đến out-of-distribution (OOD) trong các nhúng vị trí quay (RoPE) thông qua ba đóng góp chính:

1. **Giả thuyết về OOD**: Đề xuất rằng việc đào tạo không đầy đủ trong các chiều RoPE cao hơn dẫn đến các vấn đề OOD.
2. **Thuật toán tái tỷ lệ RoPE**: Một thuật toán sáng tạo sử dụng tìm kiếm tiến hóa được hướng dẫn bởi đánh giá perplexity "needle-driven" để giải quyết tình trạng đào tạo không đủ.
3. **Chiến lược đào tạo ngữ cảnh hỗn hợp**: Một chiến lược tinh chỉnh trọng số mô hình cho cả ngữ cảnh dài và ngắn, đảm bảo hiệu suất được duy trì trên cả hai loại ngữ cảnh.

LongRoPE2 đã được xác thực thông qua các thí nghiệm rộng rãi trên các mô hình như LLaMA3-8B và Phi3-mini-3.8B, đạt được chiều dài ngữ cảnh hiệu quả lên đến 128K trong khi vẫn giữ được hơn 98.5% hiệu suất của ngữ cảnh ngắn, sử dụng một lượng token đào tạo ít hơn đáng kể so với các phương pháp trước đó. Phương pháp này không chỉ giải quyết các vấn đề OOD mà còn nâng cao khả năng xử lý ngữ cảnh dài, tạo ra một bước tiến hứa hẹn trong công nghệ LLM.
