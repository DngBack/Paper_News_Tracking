# Summary of 2503.10622.pdf

# Transformers without Normalization

Bài báo "Transformers without Normalization" của Jiachen Zhu và các cộng sự giới thiệu một phương pháp mới để huấn luyện các mô hình Transformer mà không cần các lớp chuẩn hóa truyền thống, vốn được coi là thiết yếu trong các mạng nơ-ron hiện đại. Các tác giả đề xuất một sự thay thế đơn giản gọi là Dynamic Tanh (DyT), được định nghĩa là DyT(x) = tanh(αx), trong đó α là một tham số có thể học được. Phương pháp này mô phỏng hành vi của chuẩn hóa lớp bằng cách điều chỉnh đầu vào và làm giảm các giá trị cực đoan mà không cần đến các thống kê kích hoạt.

Nghiên cứu cho thấy rằng các mô hình Transformer sử dụng DyT có thể đạt được hiệu suất tương đương hoặc tốt hơn trong nhiều nhiệm vụ khác nhau, bao gồm nhận diện, sinh dữ liệu, cũng như học có giám sát và không có giám sát, mà không cần tinh chỉnh siêu tham số phức tạp. Những phát hiện này thách thức niềm tin phổ biến rằng các lớp chuẩn hóa là không thể thiếu, đồng thời cung cấp những hiểu biết mới về vai trò của chúng trong các mạng sâu.

Bài báo cũng thảo luận về hiệu quả của DyT, cho thấy rằng nó có thể giảm thời gian huấn luyện và suy diễn so với các phương pháp chuẩn hóa truyền thống. Các thí nghiệm trên nhiều kiến trúc khác nhau, bao gồm Vision Transformers và các mô hình ngôn ngữ lớn, cho thấy DyT duy trì hoặc cải thiện hiệu suất trong khi đơn giản hóa quy trình huấn luyện.

Kết luận, các tác giả lập luận rằng DyT cung cấp một lựa chọn khả thi thay thế cho các lớp chuẩn hóa, có khả năng định hình lại sự hiểu biết về sự cần thiết của chúng trong các kiến trúc học sâu.
