# Summary of 2503.10582.pdf

# VisualWebInstruct: Enhancing Multimodal Instruction Data for Vision-Language Models

Bài báo này giới thiệu **VisualWebInstruct**, một phương pháp mới nhằm cải thiện dữ liệu hướng dẫn đa phương thức cho các Mô hình Ngôn ngữ-Ảnh (VLMs) bằng cách tận dụng tìm kiếm trên web. Tác giả đã giải quyết vấn đề thiếu hụt các tập dữ liệu chất lượng cao tập trung vào lý luận bằng cách tạo ra một tập dữ liệu đa dạng, bao gồm khoảng 900,000 cặp câu hỏi-trả lời, trong đó 40% là các cặp QA hình ảnh. Tập dữ liệu này bao trùm nhiều lĩnh vực khác nhau như toán học, vật lý và tài chính, và được tạo ra thông qua một quy trình hệ thống bao gồm việc thu thập hình ảnh gốc, sử dụng tìm kiếm hình ảnh của Google và xử lý nội dung web để trích xuất và tinh chỉnh các cặp QA.

Các mô hình được tinh chỉnh trên VisualWebInstruct, đặc biệt là **MAmmoTH-VL2**, cho thấy sự cải thiện đáng kể về hiệu suất trên nhiều tiêu chuẩn đánh giá, đạt được kết quả tốt nhất trong các nhiệm vụ lý luận. Tính toàn diện của tập dữ liệu và quy trình khai thác dữ liệu hiệu quả đã góp phần nâng cao khả năng lý luận trong các VLMs, cho thấy tiềm năng của nó cho các nghiên cứu và ứng dụng trong lý luận đa phương thức trong tương lai. Tác giả dự định công khai phát hành tập dữ liệu này để mang lại lợi ích cho cộng đồng nghiên cứu.
