# Summary of 2503.10480.pdf

# Enhancing Embodied Task Planning with Dual Preference Optimization (D²PO)

Bài báo này trình bày một khung công tác mới mang tên **Dual Preference Optimization (D²PO)**, nhằm nâng cao khả năng lập kế hoạch cho các nhiệm vụ thông qua việc cải thiện mô hình thế giới. Các phương pháp truyền thống trong các mô hình ngôn ngữ-vision lớn (LVLMs) thường tập trung vào việc lựa chọn hành động mà không xem xét đầy đủ các hậu quả của những hành động đó, dẫn đến sự kém hiệu quả và sai sót trong việc thực hiện nhiệm vụ. D²PO giải quyết vấn đề này bằng cách tối ưu hóa đồng thời việc dự đoán trạng thái và lựa chọn hành động, cho phép các mô hình hiểu rõ hơn về động lực của môi trường.

Để hỗ trợ cho điều này, các tác giả đã giới thiệu một cơ chế tìm kiếm cây (tree search) tự động thu thập các quỹ đạo và dữ liệu sở thích, loại bỏ nhu cầu về chú thích của con người. Các thí nghiệm được thực hiện trên benchmark VoTa-Bench cho thấy rằng khung D²PO vượt trội hơn hẳn so với các phương pháp hiện có, bao gồm cả GPT-4o, về tỷ lệ thành công của nhiệm vụ và hiệu quả thực hiện.

## Đóng góp chính của nghiên cứu bao gồm:
1. Giới thiệu D²PO, nâng cao khả năng lập kế hoạch bằng cách tích hợp mô hình thế giới vào quy trình đào tạo.
2. Một thuật toán tìm kiếm cây hiệu quả thu thập các trải nghiệm tương tác đa dạng cho việc đào tạo.
3. Bằng chứng thực nghiệm cho thấy D²PO dẫn đến những cải tiến đáng kể về tỷ lệ thành công và hiệu quả lập kế hoạch trên nhiều mô hình khác nhau.

## Kết quả
Các kết quả chỉ ra rằng việc tích hợp mô hình thế giới vào quy trình lập kế hoạch cho phép ra quyết định tốt hơn và khả năng thích ứng trong các môi trường động, cuối cùng dẫn đến các hệ thống AI thể hiện hiệu quả hơn.
