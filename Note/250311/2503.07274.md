# Summary of 2503.07274.pdf

# Adapter Guidance Distillation (AGD)

Bài báo này giới thiệu phương pháp **Adapter Guidance Distillation (AGD)**, được thiết kế để nâng cao hiệu quả của **classifier-free guidance (CFG)** trong các mô hình khuếch tán. AGD cho phép tạo ra mẫu chất lượng cao chỉ với một lần truyền qua mỗi bước suy diễn, hiệu quả gấp đôi tốc độ lấy mẫu so với các phương pháp CFG truyền thống, vốn yêu cầu hai lần truyền. Điều này đạt được bằng cách đào tạo các adapter nhẹ, giúp xấp xỉ CFG trong khi giữ nguyên mô hình cơ sở, từ đó giảm thiểu yêu cầu tài nguyên trong quá trình đào tạo.

AGD giải quyết những điểm yếu trong các phương pháp hướng dẫn chưng cất hiện có bằng cách đào tạo trên các quỹ đạo được hướng dẫn bởi CFG thay vì các quỹ đạo khuếch tán tiêu chuẩn, điều này cải thiện hiệu suất và giảm thiểu sự không khớp giữa đào tạo và suy diễn. Phương pháp này tiết kiệm tài nguyên, cho phép chưng cất các mô hình lớn trên các GPU tiêu chuẩn, đồng thời duy trì hoặc cải thiện chất lượng mẫu so với CFG.

Các thí nghiệm rộng rãi cho thấy AGD đạt hoặc vượt qua hiệu suất của CFG trên nhiều kiến trúc khác nhau, bao gồm **Diffusion Transformer** và **Stable Diffusion**, trong khi giảm đáng kể số lượng đánh giá hàm nơ-ron (NFEs) cần thiết. Bài báo kết luận rằng AGD cung cấp một lựa chọn linh hoạt và hiệu quả hơn cho các kỹ thuật chưng cất hướng dẫn trước đó, với tiềm năng tích hợp thêm với các thuật toán hướng dẫn nâng cao và các phương pháp chưng cất khác.
