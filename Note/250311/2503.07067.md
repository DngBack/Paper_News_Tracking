# Summary of 2503.07067.pdf

```markdown
# Summary of DISTILLM-2: A Contrastive Approach Boosts the Distillation of LLMs

Bài báo này giới thiệu DISTILLM-2, một phương pháp mới cho việc chắt lọc kiến thức trong các mô hình ngôn ngữ lớn (LLMs) sử dụng cách tiếp cận tương phản nhằm nâng cao hiệu suất của các mô hình sinh viên. Các phương pháp chắt lọc truyền thống thường sử dụng cùng một hàm mất mát cho cả dữ liệu của giáo viên và sinh viên, điều này có thể hạn chế hiệu suất. DISTILLM-2 giải quyết vấn đề này bằng cách áp dụng các hàm mất mát khác nhau cho các đầu ra do giáo viên tạo ra (TGOs) và các đầu ra do sinh viên tạo ra (SGOs), tận dụng hiệu quả sự tương tác giữa các công thức mất mát và loại dữ liệu.

## Các đóng góp chính của DISTILLM-2 bao gồm:
1. **Động lực Mất mát Tương phản**: Phương pháp này sử dụng các hàm mất mát không đối xứng để tăng khả năng đúng của các phản hồi từ giáo viên trong khi giảm khả năng sai của các phản hồi từ sinh viên.
2. **Tối ưu hóa Việc Chọn Dữ liệu**: Nó giới thiệu các chiến lược để chọn dữ liệu huấn luyện phù hợp hơn với phân phối của mô hình sinh viên, cải thiện hiệu quả huấn luyện.
3. **Học Tập Thích ứng Dựa trên Chương trình**: Cách tiếp cận này điều chỉnh động các tham số hàm mất mát dựa trên độ khó của việc huấn luyện, nâng cao hiệu suất của mô hình.

Các thí nghiệm rộng rãi cho thấy DISTILLM-2 đạt được kết quả hàng đầu trong nhiều nhiệm vụ khác nhau, bao gồm theo dõi hướng dẫn, lý luận toán học và tạo mã. Phương pháp này cũng cho thấy tính linh hoạt trong các ứng dụng như căn chỉnh sở thích và chắt lọc mô hình ngôn ngữ-vision.

Tổng thể, DISTILLM-2 đại diện cho một bước tiến quan trọng trong lĩnh vực chắt lọc LLM, cung cấp một khung làm việc hiệu quả cân bằng các phức tạp trong tương tác giữa giáo viên và sinh viên, đồng thời nâng cao hiệu quả của việc huấn luyện mô hình.
```
