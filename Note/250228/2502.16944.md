# Summary of 2502.16944.pdf

# Decoupled Value Policy Optimization (DVPO)

Bài báo này giới thiệu **Decoupled Value Policy Optimization (DVPO)**, một khung công tác mới cho **Học Tăng Cường từ Phản Hồi của Con Người (RLHF)**, nhằm giải quyết những thách thức của các phương pháp **Tối Ưu Chính Sách Gần (PPO)** truyền thống. DVPO sử dụng một **Mô Hình Giá Trị Toàn Cầu (GVM)** đã được huấn luyện trước để cung cấp các ước lượng lợi nhuận theo từng token một cách ổn định, loại bỏ nhu cầu về việc huấn luyện đồng thời diễn viên và nhà phê bình, điều này thường dẫn đến độ phức tạp tính toán và sự không ổn định.

## Đóng góp của bài báo

Tác giả chỉ ra rằng việc huấn luyện trước một mô hình thưởng và một GVM là tương đương về mặt chức năng trong các tình huống không có phản hồi thưởng mới. Kết quả thực nghiệm cho thấy DVPO vượt trội hơn so với các phương pháp RLHF hiện có, đồng thời giảm đáng kể thời gian huấn luyện và mức sử dụng GPU.

## Tổng quan về kết quả và phương pháp

Khung công tác này đã được xác thực qua nhiều tiêu chuẩn khác nhau, cho thấy hiệu quả của nó trong việc điều chỉnh các mô hình ngôn ngữ lớn theo sở thích của con người mà vẫn duy trì được tính hiệu quả. Công việc trong tương lai sẽ tập trung vào việc cải thiện quy trình huấn luyện của GVM để nâng cao độ chính xác.
