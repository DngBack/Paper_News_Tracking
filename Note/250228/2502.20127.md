# Summary of 2502.20127.pdf

# Subtask-oriented Reinforced Fine-Tuning (SoRFT)

Bài báo này giới thiệu về phương pháp **Subtask-oriented Reinforced Fine-Tuning (SoRFT)**, một cách tiếp cận mới nhằm nâng cao khả năng giải quyết vấn đề của các mô hình ngôn ngữ lớn (LLMs). Các khung truyền thống thường phụ thuộc vào các mô hình thương mại tốn kém và gặp khó khăn trong việc tổng quát hóa. SoRFT giải quyết những vấn đề này bằng cách phân chia vấn đề thành các tiểu tác vụ có cấu trúc: định vị tệp, định vị chức năng, định vị dòng và tạo chỉnh sửa mã.

## Đóng góp của bài báo

Bài báo cung cấp một cái nhìn sâu sắc về quy trình đào tạo của SoRFT, bao gồm hai giai đoạn: 
1. **Fine-tuning có giám sát với mẫu từ chối**: Giai đoạn này lọc dữ liệu Chain-of-Thought (CoT) bằng cách sử dụng các câu trả lời đúng.
2. **Học tăng cường dựa trên quy tắc**: Sử dụng Proximal Policy Optimization (PPO) với phần thưởng đúng.

## Kết quả và phương pháp

Các mô hình được đào tạo bằng SoRFT đã được đánh giá trên **SWE-Bench Verified** và **SWE-Bench Lite**, đạt được hiệu suất hàng đầu trong số các mô hình mã nguồn mở, với những cải tiến đáng kể trong tỷ lệ giải quyết vấn đề. Kết quả cho thấy SoRFT không chỉ nâng cao hiệu suất mà còn cải thiện khả năng tổng quát của mô hình, cung cấp một giải pháp tiết kiệm chi phí hơn so với các giải pháp thương mại.

Bài báo cũng thảo luận về tầm quan trọng của các quy tắc phần thưởng vững chắc trong học tăng cường và nhấn mạnh tiềm năng của SoRFT trong việc tận dụng dữ liệu mã nguồn mở để phát triển hơn nữa trong các nhiệm vụ liên quan đến mã.
