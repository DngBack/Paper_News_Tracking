# Summary of 2503.02495.pdf

# Union-of-Experts: Enhancing Mixture-of-Experts Models

Bài báo này giới thiệu khung Union-of-Experts (UoE), một cải tiến cho mô hình Mixture-of-Experts (MoE) bằng cách phân tách các transformer thành các nhóm chuyên gia nhằm nâng cao hiệu suất và hiệu quả. UoE giải quyết những hạn chế trong các phương pháp MoE hiện có, chẳng hạn như tương tác giữa các chuyên gia không đủ và thiếu ứng dụng hiệu quả trong các khối attention.

## Đóng góp của bài báo

Các đổi mới chính bao gồm:

1. Phân tách cả khối MLP và khối attention thành các nhóm chuyên gia.
2. Triển khai hai mô hình định tuyến: lựa chọn dữ liệu theo từng patch và lựa chọn chuyên gia.
3. Thiết kế kiến trúc UoE với Selective Multi-Head Attention (SMHA) và Union-of-MLP-Experts (UoME).
4. Phát triển một triển khai song song cho định tuyến và tính toán nhằm tối ưu hóa hiệu quả.

## Kết quả và phương pháp

Các thí nghiệm cho thấy UoE vượt trội hơn các mô hình hiện tại trong các tác vụ như mô hình ngôn ngữ, mô hình chuỗi dài và phân loại hình ảnh, đạt được sự giảm đáng kể về perplexity và chi phí tính toán. Mô hình này thể hiện hiệu suất mạnh mẽ trên nhiều tác vụ khác nhau trong khi vẫn duy trì số lượng phép toán dấu phẩy động (FLOPs) thấp hơn so với các phương pháp hiện có. 

Các tác giả cung cấp mã nguồn để phục vụ cho nghiên cứu và ứng dụng trong các mô hình ngôn ngữ quy mô lớn.
