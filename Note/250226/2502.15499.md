# Summary of 2502.15499.pdf

```markdown
# Giới thiệu về Scale-Distribution Decoupling (SDD)

Bài báo này giới thiệu phương pháp Scale-Distribution Decoupling (SDD), được thiết kế nhằm nâng cao tính ổn định trong quá trình huấn luyện các mô hình ngôn ngữ lớn (LLMs), đặc biệt là những mô hình sử dụng Post-Norm Transformers, thường gặp phải các vấn đề như bùng nổ và suy giảm gradient. 

## Nguyên lý hoạt động của SDD

SDD hoạt động bằng cách tách biệt quy mô và phân phối của các ma trận trọng số trong các lớp kết nối đầy đủ. Phương pháp này áp dụng chuẩn hóa để điều chỉnh các hoạt động và sử dụng một vector quy mô có thể học được nhằm duy trì gradient ổn định. Cách tiếp cận này cải thiện hiệu quả tối ưu hóa và tính ổn định, đặc biệt là trong các mạng sâu.

## Kết quả thực nghiệm

Các kết quả thực nghiệm cho thấy SDD đã làm ổn định đáng kể quá trình huấn luyện trên nhiều kiến trúc LLM khác nhau, vượt trội hơn so với các kỹ thuật chuẩn hóa hiện có. Phương pháp này nhẹ nhàng, yêu cầu thay đổi tối thiểu đối với các khung hiện có, làm cho nó trở thành một giải pháp thực tiễn cho việc huấn luyện LLM.

## Phân tích và đánh giá

Bài báo cung cấp chi tiết về phương pháp, phân tích lý thuyết và các đánh giá thực nghiệm rộng rãi, chứng minh hiệu quả của SDD trong việc cải thiện sự hội tụ, khả năng tổng quát và độ bền với các biến thể hyperparameter trong cả các mô hình dày đặc và Mixture of Experts (MoE). 

## Kết luận

Tổng thể, SDD mang đến một phương pháp hứa hẹn để giải quyết những thách thức chính trong việc huấn luyện các mô hình ngôn ngữ quy mô lớn.
```
