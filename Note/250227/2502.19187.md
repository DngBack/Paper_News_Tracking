# Summary of 2502.19187.pdf

# BIG-Bench Extra Hard (BBEH): A New Benchmark for Evaluating Reasoning Capabilities of Large Language Models

Bài báo này giới thiệu BIG-Bench Extra Hard (BBEH), một chuẩn mực mới được thiết kế để đánh giá khả năng suy luận của các mô hình ngôn ngữ lớn (LLMs) vượt ra ngoài các chuẩn mực hiện có như BIG-Bench Hard (BBH). BBEH thay thế các nhiệm vụ trong BBH bằng những nhiệm vụ khó khăn hơn, yêu cầu kỹ năng suy luận nâng cao, chẳng hạn như suy luận nhiều bước, xử lý ngữ cảnh dài và phát hiện lỗi trong các dấu vết suy luận. Chuẩn mực này bao gồm 23 nhiệm vụ, mỗi nhiệm vụ có độ khó tăng đáng kể, nhằm cung cấp một thước đo chính xác hơn về khả năng suy luận tổng quát của LLMs.

## Đóng góp của bài báo

Bài báo nhấn mạnh những hạn chế của các LLM hiện tại trong việc xử lý các nhiệm vụ suy luận đa dạng và nhấn mạnh sự cần thiết của nghiên cứu liên tục để nâng cao khả năng suy luận của chúng. Các tác giả cũng cung cấp mô tả chi tiết về các nhiệm vụ trong BBEH, thể hiện sự phức tạp và kỹ năng suy luận cần thiết cho từng nhiệm vụ.

## Kết quả và phương pháp

Kết quả đánh giá cho thấy ngay cả những mô hình hoạt động tốt nhất cũng đạt được độ chính xác trung bình hài thấp (9.8% cho các mô hình đa mục đích và 44.8% cho các mô hình chuyên về suy luận), cho thấy còn nhiều không gian để cải thiện. Chuẩn mực này hiện đã được công khai để phục vụ cho nghiên cứu và đánh giá thêm, nhằm mở rộng ranh giới của việc đánh giá khả năng suy luận của LLMs.
