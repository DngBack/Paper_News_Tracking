# Summary of 2502.19414.pdf

# Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation

Bài viết trước in "Can Language Models Falsify? Evaluating Algorithmic Reasoning with Counterexample Creation" của Shiven Sinha và các cộng sự khám phá tiềm năng của các Mô hình Ngôn ngữ (LMs) trong việc phát hiện khoa học, đặc biệt là trong bối cảnh bác bỏ các giả thuyết. Các tác giả lập luận rằng các tiêu chuẩn hiện tại chủ yếu đánh giá khả năng của LMs trong việc tạo ra các giải pháp, thay vì khả năng thách thức các giải pháp sai.

Họ đề xuất một tiêu chuẩn mới mang tên REFUTE, tập trung vào việc tạo ra các phản ví dụ cho các giải pháp thuật toán không chính xác. 

Nghiên cứu cho thấy rằng ngay cả những LMs tiên tiến như o3-mini của OpenAI cũng gặp khó khăn trong việc tạo ra các phản ví dụ, chỉ thành công trong việc tạo ra phản ví dụ cho chưa đến 9% các giải pháp sai, mặc dù có thể giải quyết tới 48% các bài toán. Điều này chỉ ra một khoảng cách đáng kể trong khả năng lý luận của LMs, nhấn mạnh sự cần thiết của các tiêu chuẩn đánh giá khả năng bác bỏ các tuyên bố. 

Các tác giả hy vọng rằng công trình của họ sẽ truyền cảm hứng cho các nghiên cứu tiếp theo nhằm nâng cao khả năng lý luận phản chiếu và khả năng tự cải thiện của LMs, điều này rất quan trọng cho việc điều tra khoa học đáng tin cậy.
