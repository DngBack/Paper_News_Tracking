# Summary of 2502.20811.pdf

# HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models

Bài báo "HAIC: Improving Human Action Understanding and Generation with Better Captions for Multi-modal Large Language Models" của Xiao Wang và các cộng sự đề cập đến những hạn chế của các Mô hình Ngôn ngữ Đa phương thức Lớn (MLLMs) trong việc hiểu các hành động của con người trong video do thiếu dữ liệu chất lượng cao. Để giải quyết vấn đề này, các tác giả đã đề xuất một quy trình chú thích dữ liệu hai giai đoạn. Giai đoạn đầu tiên là thu thập video có các hành động của con người rõ ràng từ internet, sau đó chú thích chúng theo định dạng chú thích chuẩn, chi tiết các thuộc tính và hành động của con người theo trình tự thời gian. Quá trình này tạo ra hai tập dữ liệu: HAICTrain, chứa 126K cặp video-chú thích cho việc huấn luyện, và HAICBench, bao gồm 500 cặp video-chú thích được chú thích thủ công và 1,400 cặp câu hỏi-đáp để đánh giá.

## Đóng góp của bài báo

Bài báo nhấn mạnh tầm quan trọng của các chú thích chi tiết, chất lượng cao trong việc cải thiện hiệu suất của MLLMs trong các nhiệm vụ hiểu video, đặc biệt là trong các bối cảnh liên quan đến hành động và tương tác của con người. Các kết quả thực nghiệm cho thấy việc huấn luyện với HAICTrain đã nâng cao đáng kể khả năng hiểu hành động của con người trên nhiều tiêu chuẩn đánh giá và cải thiện khả năng tạo video từ văn bản.

## Kết quả và phương pháp

Các tập dữ liệu được công khai để thúc đẩy nghiên cứu thêm trong lĩnh vực này. Nghiên cứu cũng thừa nhận một số hạn chế, chẳng hạn như khả năng có thiên lệch trong các video nguồn và nhu cầu tích hợp với dữ liệu âm thanh để có cái nhìn toàn diện hơn về hành động của con người.
