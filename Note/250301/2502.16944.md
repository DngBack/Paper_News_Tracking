# Summary of 2502.16944.pdf

# Decoupled Value Policy Optimization (DVPO)

Bài báo này giới thiệu về **Decoupled Value Policy Optimization (DVPO)**, một khung công tác mới cho **Học Tăng Cường từ Phản Hồi của Con Người (RLHF)**, nhằm giải quyết những thách thức mà các phương pháp **Proximal Policy Optimization (PPO)** truyền thống gặp phải. 

## Tóm tắt nội dung

DVPO sử dụng một **Mô Hình Giá Trị Toàn Cầu (GVM)** đã được huấn luyện trước để cung cấp các ước lượng trả về theo từng token một cách ổn định, loại bỏ nhu cầu huấn luyện đồng thời giữa diễn viên và nhà phê bình, điều này thường dẫn đến độ phức tạp tính toán và sự không ổn định.

## Đóng góp của bài báo

Tác giả chứng minh rằng việc huấn luyện trước một mô hình thưởng và một GVM sẽ tạo ra các tín hiệu giám sát tương đương khi không có phần thưởng thực tế mới. Kết quả thực nghiệm cho thấy DVPO vượt trội hơn so với các phương pháp RLHF hiện có, đồng thời giảm đáng kể thời gian huấn luyện và mức sử dụng GPU.

## Tóm tắt kết quả và phương pháp

Khung công tác này đã được xác thực qua nhiều tiêu chuẩn khác nhau, cho thấy hiệu quả của nó trong việc điều chỉnh các mô hình ngôn ngữ lớn theo sở thích của con người, đồng thời nâng cao hiệu quả huấn luyện. Công việc trong tương lai sẽ tập trung vào việc cải thiện quy trình huấn luyện của GVM để đạt được độ chính xác dự đoán tốt hơn.
