# Summary of 2502.20082.pdf

# LongRoPE2: Enhancing Context Windows in Large Language Models

Bài viết này giới thiệu LongRoPE2, một phương pháp mới được phát triển nhằm cải thiện kích thước ngữ cảnh hiệu quả của các mô hình ngôn ngữ lớn (LLMs) mà vẫn duy trì hiệu suất trên các ngữ cảnh ngắn. LongRoPE2 giải quyết các vấn đề liên quan đến việc ra ngoài phân phối (OOD) trong các nhúng vị trí quay (RoPE) thông qua ba đóng góp chính:

1. **Giả thuyết về đào tạo không đầy đủ**: Đưa ra giả thuyết rằng việc đào tạo không đủ trong các chiều RoPE cao hơn dẫn đến các vấn đề OOD.
2. **Thuật toán tái tỷ lệ RoPE hiệu quả**: Phát triển một thuật toán tái tỷ lệ RoPE sử dụng tìm kiếm tiến hóa được hướng dẫn bởi độ khó "needle-driven" để giải quyết vấn đề đào tạo không đủ.
3. **Phương pháp đào tạo ngữ cảnh hỗn hợp**: Áp dụng một phương pháp đào tạo ngữ cảnh hỗn hợp để tinh chỉnh trọng số mô hình cho các chuỗi ngữ cảnh dài trong khi vẫn bảo tồn hiệu suất cho ngữ cảnh ngắn.

LongRoPE2 đã được xác thực thông qua các thí nghiệm rộng rãi trên các mô hình như LLaMA3-8B và Phi3-mini-3.8B, đạt được độ dài ngữ cảnh hiệu quả 128K trong khi vẫn duy trì hơn 98.5% hiệu suất cho ngữ cảnh ngắn, chỉ sử dụng 10 tỷ token—ít hơn đáng kể so với các phương pháp trước đó. Phương pháp này kết hợp RoPE đã được tái tỷ lệ cho các ngữ cảnh dài với RoPE gốc cho các ngữ cảnh ngắn, tối ưu hóa hiệu suất trên nhiều tiêu chuẩn khác nhau. Nhìn chung, LongRoPE2 đại diện cho một bước tiến quan trọng trong khả năng của LLM trong việc xử lý các ngữ cảnh dài một cách hiệu quả.
