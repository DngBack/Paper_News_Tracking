# Summary of 2503.02972.pdf

# LINGOLY-TOO: A Benchmark for Evaluating Reasoning Capabilities of Large Language Models

Bài báo này giới thiệu LINGOLY-TOO, một bộ chuẩn được thiết kế để đánh giá khả năng suy luận của các Mô hình Ngôn ngữ Lớn (LLMs) trong khi giảm thiểu ảnh hưởng của việc ghi nhớ. Các tác giả đã phát triển một khung làm việc sử dụng phương pháp lập khuôn ngôn ngữ và làm mờ chính tả để tạo ra các biến thể vấn đề đa dạng từ các câu hỏi của Olympiad Ngôn ngữ hiện có. Cách tiếp cận này nhằm đảm bảo rằng các mô hình được kiểm tra trên dữ liệu chưa thấy, từ đó cung cấp một đánh giá chính xác hơn về khả năng suy luận của chúng.

## Đóng góp của bài báo

Bộ chuẩn bao gồm 27,325 câu hỏi suy luận ngôn ngữ được tạo ra thông qua một quy trình làm mờ có hệ thống. Các tác giả phát hiện rằng các mô hình tiên tiến, bao gồm Claude 3.7 Sonnet và GPT-4, gặp khó khăn với các nhiệm vụ suy luận nâng cao, cho thấy sự giảm hiệu suất đáng kể trên các vấn đề đã được làm mờ so với các dạng gốc của chúng. Nghiên cứu cũng chỉ ra rằng các LLM thể hiện độ chính xác khác nhau trên các biến thể vấn đề khác nhau, cho thấy rằng việc tiếp xúc trước với dữ liệu huấn luyện có thể dẫn đến việc đánh giá quá cao khả năng suy luận.

## Kết quả và phương pháp

Bài báo cũng thảo luận về những tác động của việc làm mờ đối với hiệu suất của con người, tiết lộ rằng trong khi các bước suy luận vẫn không thay đổi, việc làm mờ có thể khiến các vấn đề trở nên khó khăn hơn, dẫn đến điểm số thấp hơn trong số các người tham gia. Các tác giả kết luận rằng LINGOLY-TOO cung cấp một thước đo vững chắc về khả năng suy luận mà ít bị ảnh hưởng bởi các hiệu ứng tiếp xúc dữ liệu, góp phần vào những nỗ lực đang diễn ra để hiểu rõ hơn về quy trình suy luận của các LLM.
