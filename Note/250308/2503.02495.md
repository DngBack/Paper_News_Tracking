# Summary of 2503.02495.pdf

# Union-of-Experts: Enhancing Mixture-of-Experts Models

Bài báo này giới thiệu khung làm việc Union-of-Experts (UoE), một cải tiến cho mô hình Mixture-of-Experts (MoE) thông qua việc nâng cao tương tác giữa các chuyên gia và mở rộng ứng dụng của nó đến các khối attention. UoE phân tách các transformer thành các nhóm chuyên gia và thực hiện việc định tuyến chọn lọc cho dữ liệu đầu vào và các chuyên gia, dẫn đến hiệu suất và hiệu quả tốt hơn.

## Đóng góp của bài báo

Các đổi mới chính của UoE bao gồm:
- Phân tách chuyên gia tương đương trong cả khối MLP và khối attention.
- Hai mô hình định tuyến: chọn lọc dữ liệu theo patch và chọn lọc chuyên gia.
- Thực hiện song song giữa định tuyến và tính toán.

## Kết quả và phương pháp

Các thí nghiệm cho thấy UoE vượt trội hơn so với các mô hình hiện có, đạt được độ perplexity thấp hơn trong các tác vụ mô hình ngôn ngữ và độ chính xác cao hơn trong phân loại hình ảnh, đồng thời giảm chi phí tính toán. Cụ thể, UoE đạt được mức giảm 2.38 trong độ perplexity và cải thiện 1.75% độ chính xác trong phân loại hình ảnh so với các mô hình tốt nhất hiện có, với FLOPs thấp hơn đáng kể. Kiến trúc của mô hình cho phép xử lý hiệu quả các chuỗi dài, làm cho nó phù hợp với các ứng dụng quy mô lớn.

## Kết luận

Các tác giả kết luận rằng UoE giải quyết hiệu quả các hạn chế của các kiến trúc MoE và transformer truyền thống, cho thấy tiềm năng của nó cho nghiên cứu và ứng dụng trong các mô hình ngôn ngữ lớn trong tương lai. Mã nguồn cho UoE đã được công khai để phục vụ cho việc khám phá thêm.
