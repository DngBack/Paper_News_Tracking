# Summary of 2503.02682.pdf

# Meta Plan Optimization (MPO): Enhancing Planning Capabilities of LLM Agents

Bài báo này trình bày khung Meta Plan Optimization (MPO), được thiết kế để cải thiện khả năng lập kế hoạch của các tác nhân mô hình ngôn ngữ lớn (LLM) trong các nhiệm vụ tương tác. Các tác nhân dựa trên LLM hiện tại gặp phải những thách thức như ảo tưởng lập kế hoạch và cần phải huấn luyện lại với mỗi tác nhân mới. MPO giải quyết những vấn đề này bằng cách tích hợp các kế hoạch meta cấp cao, cung cấp hướng dẫn rõ ràng cho việc thực hiện nhiệm vụ, cho phép tối ưu hóa liên tục dựa trên phản hồi từ hiệu suất của tác nhân.

MPO bao gồm một bộ lập kế hoạch meta, tạo ra các chiến lược trừu tượng cho việc hoàn thành nhiệm vụ, tách rời khỏi các chi tiết môi trường cụ thể. Cách tiếp cận này cho phép bộ lập kế hoạch meta được tinh chỉnh bằng các kỹ thuật học tăng cường, đặc biệt thông qua việc học sở thích trên các cặp kế hoạch meta đối lập. Khung này đã được đánh giá trên hai tiêu chuẩn: ALFWorld và ScienceWorld, cho thấy sự cải thiện hiệu suất đáng kể—lên đến 100%—so với các phương pháp hiện có.

## Đóng góp chính của nghiên cứu bao gồm:
1. Giới thiệu MPO để nâng cao hiệu suất của tác nhân LLM theo cách cắm và chạy.
2. Các thí nghiệm rộng rãi cho thấy sự cải thiện đáng kể trong hiệu quả hoàn thành nhiệm vụ và khả năng tổng quát.
3. Phân tích cho thấy rằng các kế hoạch meta do MPO tạo ra hiệu quả hơn so với các kế hoạch truyền thống, nâng cao độ chính xác, khả năng thực hiện và tiêu chuẩn hóa.

Bài báo kết luận rằng MPO đại diện cho một bước tiến quan trọng trong khả năng lập kế hoạch của tác nhân, mở đường cho các phát triển trong trí tuệ nhân tạo tổng quát trong tương lai. Tuy nhiên, nó cũng thừa nhận những hạn chế, chẳng hạn như sự phụ thuộc vào một mô hình cơ sở cụ thể và khả năng tối ưu hóa thêm thông qua các phương pháp lấy mẫu tinh vi hơn.
