# Summary of 2503.02846.pdf

# MASK-DPO: Generalizable Fine-Grained Factuality Alignment of LLMs

Bài báo "MASK-DPO: Generalizable Fine-Grained Factuality Alignment of LLMs" giới thiệu một phương pháp mới mang tên Mask-DPO, nhằm giảm thiểu hiện tượng "hallucination" trong các mô hình ngôn ngữ lớn (LLMs). Các phương pháp truyền thống thường trộn lẫn thông tin đúng và sai trong các phản hồi, dẫn đến việc huấn luyện không hiệu quả. Mask-DPO giải quyết vấn đề này bằng cách sử dụng tính chính xác của câu ở cấp độ câu như một tín hiệu mặt nạ, cho phép mô hình học chỉ từ các câu đúng về mặt thực tế trong các mẫu ưa thích, trong khi bỏ qua các câu sai trong các mẫu không ưa thích.

## Đóng góp của bài báo

Bài báo cung cấp một cái nhìn sâu sắc về cách mà Mask-DPO có thể cải thiện tính chính xác của các phản hồi từ LLM, đồng thời mở ra hướng nghiên cứu mới về việc mở rộng khả năng căn chỉnh tính chính xác.

## Kết quả và phương pháp

Kết quả thử nghiệm cho thấy rằng Mask-DPO nâng cao đáng kể tính chính xác của các phản hồi từ LLM, đạt được sự cải thiện điểm số từ 49.19% lên 77.53% trên tập kiểm tra ANAH, vượt qua các mô hình khác. Phương pháp này cũng thể hiện hiệu suất mạnh mẽ trên các tập dữ liệu ngoài miền. Nghiên cứu còn khám phá các thuộc tính tổng quát của Mask-DPO, cho thấy rằng việc tăng cường sự đa dạng của các chủ đề trong dữ liệu huấn luyện có lợi hơn so với việc tăng số lượng câu hỏi.

Các tác giả giả thuyết rằng các LLM phát triển một đồ thị kiến thức cụ thể trong quá trình huấn luyện, nơi mà sự căn chỉnh thực tế trên một chủ đề có thể ảnh hưởng tích cực đến các chủ đề liên quan. Bài báo kết luận rằng Mask-DPO hiệu quả trong việc căn chỉnh kiến thức nội bộ của các LLM và khuyến khích nghiên cứu trong tương lai về việc mở rộng căn chỉnh tính chính xác.
