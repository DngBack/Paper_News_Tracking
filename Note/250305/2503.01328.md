# Summary of 2503.01328.pdf

# PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization

Bài báo "PipeOffload: Improving Scalability of Pipeline Parallelism with Memory Optimization" của Xinyi Wan và các cộng sự giải quyết các vấn đề về khả năng mở rộng của pipeline parallelism (PP) trong việc huấn luyện các mô hình ngôn ngữ lớn (LLMs) do tiêu thụ bộ nhớ kích hoạt cao. Tác giả đề xuất một chiến lược offload bộ nhớ cho phép giảm đáng kể bộ nhớ kích hoạt, từ đó cải thiện khả năng mở rộng của PP.

## Đóng góp của bài báo

Bài báo nhấn mạnh rằng việc tăng số lượng giai đoạn PP sẽ làm giảm số lớp trên mỗi thiết bị, nhưng lại yêu cầu nhiều microbatches đang hoạt động hơn, giữ cho nhu cầu bộ nhớ kích hoạt tổng thể vẫn cao. Tác giả chứng minh rằng bằng cách offload bộ nhớ sang máy chủ, họ có thể giảm hiệu quả bộ nhớ kích hoạt đỉnh, đạt được tới 19% tăng tốc trong quá trình huấn luyện với mức tiêu thụ bộ nhớ thấp hơn so với các phương pháp truyền thống.

## Phương pháp thực hiện

Các phương pháp được đề xuất bao gồm một chiến lược xen kẽ tổng quát và một chiến lược lặp lại đồng nhất, ưu tiên offload các kích hoạt có thời gian sống dài hơn. Tác giả cũng thảo luận về các đánh đổi giữa bộ nhớ và thông lượng, trình bày một cách tiếp cận linh hoạt để tối ưu hóa hiệu suất dựa trên yêu cầu của hệ thống.

## Kết quả và phương pháp thực hiện

Chi tiết thực hiện nhấn mạnh việc lập lịch hiệu quả cho các quy trình offloading và reloading để giảm thiểu overhead. Các thí nghiệm được thực hiện trên nhiều mô hình cho thấy rằng các phương pháp của họ giảm đáng kể bộ nhớ kích hoạt trong khi vẫn duy trì thông lượng. Kết quả cho thấy PP được cải thiện có thể là một lựa chọn hấp dẫn thay thế cho tensor parallelism, giúp việc huấn luyện các mô hình lớn hơn trở nên khả thi mà trước đây bị hạn chế bởi các giới hạn bộ nhớ. Việc triển khai đã được mã nguồn mở để phục vụ cho nghiên cứu và ứng dụng tiếp theo.
