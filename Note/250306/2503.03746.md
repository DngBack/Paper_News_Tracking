# Summary of 2503.03746.pdf

# Process-based Self-Rewarding Language Models (PSRLM)

Bài báo này giới thiệu một phương pháp mới mang tên **Process-based Self-Rewarding Language Models (PSRLM)**, nhằm nâng cao hiệu suất của các Mô hình Ngôn ngữ Lớn (LLMs) trong các nhiệm vụ lý luận toán học. Các phương pháp tự thưởng truyền thống gặp phải những hạn chế, đặc biệt trong các tình huống lý luận phức tạp, dẫn đến sự suy giảm hiệu suất. Khung PSRLM giới thiệu quy trình lý luận và đánh giá từng bước, cho phép LLM tạo ra và đánh giá đầu ra của chúng một cách lặp đi lặp lại.

## Đóng góp của bài báo
Các thành phần chính của PSRLM bao gồm:
1. **LLM-as-a-Judge theo từng bước**: Mô hình đánh giá từng bước lý luận, cung cấp các tín hiệu thưởng chính xác hơn.
2. **Tối ưu hóa sở thích theo từng bước**: Kỹ thuật này tinh chỉnh khả năng của mô hình trong việc phân biệt giữa các bước lý luận tốt hơn và kém hơn, từ đó nâng cao khả năng học hỏi từ các đầu ra của chính nó.

## Kết quả và phương pháp
Các thí nghiệm được thực hiện trên nhiều bộ dữ liệu toán học khác nhau cho thấy PSRLM cải thiện đáng kể hiệu suất của LLM so với các phương pháp tự thưởng truyền thống. Kết quả cho thấy quy trình tự thưởng lặp đi lặp lại ở cấp độ từng bước dẫn đến khả năng lý luận tốt hơn, gợi ý rằng LLM có thể vượt qua hiệu suất của con người trong các nhiệm vụ lý luận phức tạp. Nghiên cứu nhấn mạnh tầm quan trọng của việc đánh giá chi tiết và học tập lặp đi lặp lại trong việc nâng cao khả năng của LLM.
