# Summary of 2503.00329.pdf

# ABC: An Open-Source Multimodal Embedding Model

Bài báo này giới thiệu về **ABC**, một mô hình nhúng đa phương thức mã nguồn mở, giúp người dùng kiểm soát tốt hơn các biểu diễn hình ảnh bằng cách tích hợp các hướng dẫn ngôn ngữ tự nhiên với các đặc trưng hình ảnh. Khác với các mô hình dựa trên CLIP hiện có, vốn nhúng độc lập hình ảnh và văn bản, ABC sử dụng một mô hình ngôn ngữ-vision (VLM) làm nền tảng để tạo ra sự tương tác sâu hơn giữa các phương thức. Điều này dẫn đến hiệu suất cải thiện trong các nhiệm vụ yêu cầu hiểu biết tinh vi, chẳng hạn như truy xuất hình ảnh sang văn bản, phân loại và trả lời câu hỏi hình ảnh (VQA).

## Đóng góp của bài báo

ABC vượt trội hơn các mô hình khác trong các nhiệm vụ không cần huấn luyện (zero-shot) trên các bộ dữ liệu chuẩn như MSCOCO và Massive Multimodal Embedding Benchmark (MMEB). Các tác giả cũng giới thiệu **CtrlBench**, một bộ dữ liệu chuẩn được thiết kế để đánh giá khả năng của mô hình trong việc tuân theo các hướng dẫn ngôn ngữ tự nhiên cho các nhiệm vụ truy xuất. 

Quá trình huấn luyện cho ABC được chia thành hai giai đoạn: giai đoạn tiền huấn luyện tốn nhiều tài nguyên tính toán sử dụng học đối kháng với các mẫu tiêu cực đã khai thác, tiếp theo là giai đoạn tinh chỉnh hướng dẫn nhẹ nhàng hơn.

## Tóm tắt kết quả và phương pháp

Các đóng góp chính của bài báo bao gồm sự phát triển của ABC, một phương pháp để điều chỉnh các VLM thành các mô hình nhúng đa phương thức tiên tiến, và sự giới thiệu của CtrlBench để đo lường khả năng truy xuất theo hướng dẫn. Các phát hiện nhấn mạnh tầm quan trọng của việc lựa chọn dữ liệu huấn luyện, kích thước lô và nền tảng VLM trong việc đạt được hiệu suất tối ưu.
