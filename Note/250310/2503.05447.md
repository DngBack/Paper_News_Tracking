# Summary of 2503.05447.pdf

# Linear-MoE: Enhancing Efficiency and Scalability in Large-Scale Models

Bài báo này giới thiệu về Linear-MoE, một hệ thống kết hợp Mô hình Chuỗi Tuyến tính (Linear Sequence Modeling - LSM) với Mixture-of-Experts (MoE) nhằm nâng cao hiệu quả và khả năng mở rộng của các mô hình quy mô lớn. Linear-MoE bao gồm hai hệ thống con chính: 

1. **Hệ thống Mô hình**: Hỗ trợ nhiều phương pháp LSM khác nhau như attention tuyến tính, mô hình không gian trạng thái và RNN tuyến tính.
2. **Hệ thống Đào tạo**: Sử dụng các kỹ thuật song song tiên tiến, đặc biệt là Song song Chuỗi (Sequence Parallelism), để tối ưu hóa quá trình đào tạo cho các chuỗi đầu vào dài.

Các tác giả nhấn mạnh những lợi ích của việc kết hợp mô hình phức tạp tuyến tính của LSM với sự kích hoạt thưa thớt của MoE, dẫn đến hiệu suất cải thiện trong khi vẫn quản lý được chi phí tính toán. Họ cũng khám phá các mô hình lai kết hợp các lớp Linear-MoE với các lớp Transformer-MoE tiêu chuẩn để tăng cường tính linh hoạt và hiệu suất, đặc biệt trong các nhiệm vụ yêu cầu khả năng hồi tưởng mạnh mẽ.

Các đánh giá thực nghiệm trên hai loạt mô hình (A0.3B-2B và A1B-7B) cho thấy Linear-MoE đạt được những cải tiến đáng kể về hiệu quả và hiệu suất cạnh tranh trên nhiều tiêu chuẩn khác nhau. Hệ thống được thiết kế để có thể mở rộng, cho phép cải tiến trong tương lai và tích hợp các kỹ thuật mô hình mới. 

Tổng thể, Linear-MoE cho thấy tiềm năng như một kiến trúc mô hình nền tảng thế hệ tiếp theo, mặc dù vẫn còn những thách thức trong việc điều chỉnh siêu tham số và khả năng mở rộng cho các mô hình cực lớn.
