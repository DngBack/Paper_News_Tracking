# Summary of 2503.02130.pdf

# Forgetting Transformer (FoX)

Bài báo này giới thiệu về Forgetting Transformer (FoX), một mô hình mới tích hợp cổng quên vào kiến trúc Transformer nhằm nâng cao hiệu suất trong các tác vụ có ngữ cảnh dài. Cổng quên cho phép giảm trọng số các điểm chú ý không chuẩn hóa theo cách phụ thuộc vào dữ liệu, cải thiện khả năng của mô hình trong việc quản lý các chuỗi dài.

## Đóng góp của bài báo

FoX vượt trội hơn so với các Transformer truyền thống trong việc mô hình hóa ngôn ngữ có ngữ cảnh dài và nhiều tác vụ hạ nguồn khác, đồng thời duy trì hiệu suất tương đương trong các tác vụ hạ nguồn có ngữ cảnh dài. Đặc biệt, mô hình này không yêu cầu nhúng vị trí và tương thích với thuật toán FlashAttention.

## Kết quả và phương pháp

Các tác giả đã giới thiệu một thiết kế "Pro" block, kết hợp các đặc điểm từ các mô hình hồi tiếp, giúp tăng cường đáng kể hiệu suất. Bài báo bao gồm các nghiên cứu thực nghiệm rộng rãi cho thấy những lợi thế của FoX so với cả Transformer tiêu chuẩn và các mô hình hồi tiếp như Mamba-2, HGRN2 và DeltaNet. Kết quả cho thấy FoX xuất sắc trong các tác vụ truy xuất ngữ cảnh dài và thể hiện hiệu suất vượt trội trong các tác vụ ngữ cảnh ngắn.

## Kết luận

Bài báo kết luận rằng nghiên cứu trong tương lai nên khám phá quy mô mô hình lớn hơn và ứng dụng của cơ chế Chú ý Quên trong các bối cảnh không nguyên nhân. Các tác giả cũng cung cấp mã nguồn của họ để phục vụ cho các thí nghiệm tiếp theo.
