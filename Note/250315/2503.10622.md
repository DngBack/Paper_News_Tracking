# Summary of 2503.10622.pdf

# Transformers without Normalization

Bài báo "Transformers without Normalization" của Jiachen Zhu và các cộng sự giới thiệu một phương pháp mới để huấn luyện các mô hình Transformer mà không cần các lớp chuẩn hóa truyền thống, thường được coi là thiết yếu cho hiệu suất. Tác giả đã giới thiệu Dynamic Tanh (DyT), một phép toán đơn giản theo từng phần tử được định nghĩa là DyT(x) = tanh(αx), trong đó α là một tham số có thể học được. Phương pháp này mô phỏng hành vi của các lớp chuẩn hóa bằng cách điều chỉnh đầu vào và làm giảm các giá trị cực đoan mà không cần đến thống kê kích hoạt.

Nghiên cứu cho thấy rằng các mô hình Transformer sử dụng DyT có thể đạt được hiệu suất tương đương hoặc vượt trội trong nhiều nhiệm vụ khác nhau, bao gồm nhận diện, sinh dữ liệu, cũng như học có giám sát và không có giám sát, mà không cần tinh chỉnh siêu tham số phức tạp. Những phát hiện này thách thức niềm tin phổ biến rằng các lớp chuẩn hóa là không thể thiếu trong các kiến trúc học sâu, đồng thời cung cấp những hiểu biết mới về vai trò của chúng.

Bài báo cũng thảo luận về bối cảnh lịch sử của các lớp chuẩn hóa, những lợi ích thực nghiệm của chúng, và sự xuất hiện của DyT như một sự thay thế đơn giản. Các tác giả đã xác thực hiệu quả của DyT thông qua các thí nghiệm rộng rãi trên nhiều kiến trúc khác nhau, bao gồm Vision Transformers và các mô hình ngôn ngữ, cho thấy rằng DyT có thể cải thiện tốc độ huấn luyện và duy trì hiệu suất của mô hình.

Tóm lại, nghiên cứu này gợi ý rằng DyT có thể là một lựa chọn khả thi thay thế cho các lớp chuẩn hóa trong các mô hình Transformer, thúc đẩy việc xem xét lại sự cần thiết của chúng trong các mạng nơ-ron hiện đại.
