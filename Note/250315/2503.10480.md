# Summary of 2503.10480.pdf

# Dual Preference Optimization (D²PO): Enhancing Embodied Task Planning

Bài báo này trình bày một khung công tác mới mang tên **Dual Preference Optimization (D²PO)**, nhằm nâng cao việc lập kế hoạch nhiệm vụ thông qua việc cải thiện mô hình thế giới. Các phương pháp truyền thống trong các mô hình ngôn ngữ-vision lớn (LVLMs) thường tập trung vào việc lựa chọn hành động mà không xem xét đầy đủ các hậu quả của những hành động đó, dẫn đến sự không hiệu quả và sai sót trong việc thực hiện nhiệm vụ. D²PO giải quyết vấn đề này bằng cách tối ưu hóa đồng thời việc dự đoán trạng thái và lựa chọn hành động thông qua việc học sở thích, cho phép các mô hình hiểu rõ hơn về động lực môi trường.

Để tạo điều kiện cho việc đào tạo, các tác giả đã giới thiệu một cơ chế tìm kiếm cây (tree search) tự động thu thập các quỹ đạo và dữ liệu sở thích, loại bỏ nhu cầu về chú thích của con người. Các thí nghiệm được thực hiện trên benchmark VoTa-Bench cho thấy khung D²PO vượt trội hơn hẳn so với các phương pháp hiện có, bao gồm cả GPT-4o, về tỷ lệ thành công của nhiệm vụ và hiệu quả thực hiện.

## Đóng góp chính của nghiên cứu bao gồm:
1. Giới thiệu D²PO để tối ưu hóa cả việc dự đoán trạng thái và lựa chọn hành động.
2. Một thuật toán tìm kiếm cây cho việc thu thập dữ liệu hiệu quả mà không cần sự can thiệp của con người.
3. Bằng chứng cho thấy việc mô hình hóa thế giới nâng cao khả năng lập kế hoạch, dẫn đến hiệu suất tốt hơn trong các môi trường động.

## Tóm tắt kết quả và phương pháp
Kết quả cho thấy D²PO không chỉ cải thiện tỷ lệ thành công của nhiệm vụ mà còn nâng cao hiệu quả của các chuỗi hành động, chứng tỏ tiềm năng của nó cho các ứng dụng thực tế trong trí tuệ nhân tạo thể hiện. Tuy nhiên, các tác giả cũng thừa nhận những hạn chế như khoảng cách giữa mô phỏng và thực tế (sim-to-real gap) và yêu cầu tính toán cao của quy trình thu thập dữ liệu của họ.
