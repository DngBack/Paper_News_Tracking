# Summary of 2503.11207.pdf

# Evaluating Large Reasoning Models under Perceptual Uncertainty

Bài viết này đánh giá hiệu suất của hai Mô Hình Lý Luận Lớn (LRMs) tiên tiến, o3-mini của OpenAI và DeepSeek R1, trong các nhiệm vụ lý luận tương tự dưới điều kiện không chắc chắn về cảm nhận, sử dụng Ma Trận Tiến Bộ Raven làm tiêu chuẩn. Các nhà nghiên cứu đã giới thiệu tập dữ liệu I-RAVEN-X, mở rộng từ I-RAVEN gốc bằng cách kết hợp các thuộc tính gây nhiễu và làm mượt các phân phối đầu vào để mô phỏng cảm nhận thị giác không hoàn hảo.

## Đóng góp của bài viết
Bài viết cung cấp cái nhìn sâu sắc về cách mà các Mô Hình Lý Luận Lớn hoạt động trong các tình huống không chắc chắn, đồng thời nhấn mạnh tầm quan trọng của việc phát triển các mô hình lý luận mạnh mẽ hơn. 

## Kết quả
Kết quả cho thấy có sự giảm sút đáng kể về độ chính xác của cả hai LRM khi đối mặt với sự không chắc chắn về cảm nhận. Cụ thể, độ chính xác của o3-mini giảm từ 86.6% xuống 17.0%, trong khi DeepSeek R1 giảm từ 80.6% xuống 23.2% trên tập dữ liệu I-RAVEN-X khó hơn. Ngược lại, một mô hình suy diễn xác suất kết hợp thần kinh, ARLC, duy trì độ chính xác cao 88.0% trong các điều kiện tương tự, cho thấy khả năng chống chịu tốt trước sự không chắc chắn.

## Phương pháp
Nghiên cứu sử dụng các phương pháp phân tích để đánh giá hiệu suất của các mô hình trong các tình huống khác nhau, từ đó rút ra kết luận về khả năng của chúng trong việc xử lý các nhiệm vụ lý luận dưới điều kiện không chắc chắn.

## Kết luận
Những phát hiện này chỉ ra rằng mặc dù các LRM hoạt động tốt trong các điều kiện lý tưởng, nhưng chúng gặp khó khăn đáng kể khi phải đối mặt với sự không chắc chắn về cảm nhận. Điều này nhấn mạnh sự cần thiết phải phát triển thêm các mô hình lý luận mạnh mẽ hơn. Nghiên cứu gợi ý rằng các phương pháp kết hợp thần kinh có thể cung cấp một lựa chọn đáng tin cậy hơn cho các nhiệm vụ lý luận liên quan đến sự không chắc chắn.
