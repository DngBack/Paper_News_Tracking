# Summary of 2503.06553.pdf

# ProJudgeBench: A Benchmark for Evaluating Multi-Modal Large Language Models as Process Judges

Bài báo này giới thiệu ProJudgeBench, một bộ chuẩn được thiết kế để đánh giá khả năng của các mô hình ngôn ngữ lớn đa phương thức (MLLMs) trong vai trò là người đánh giá quy trình trong giải quyết vấn đề khoa học. Bộ chuẩn này bao gồm 2,400 trường hợp thử nghiệm và hơn 50,000 chú thích cấp độ bước trong bốn lĩnh vực khác nhau, cho phép phân tích lỗi chi tiết. ProJudgeBench giải quyết những hạn chế của các mô hình hiện có bằng cách cung cấp các mẫu lỗi đa dạng và thực tế cùng với các chú thích của con người cho từng bước.

Để nâng cao khả năng đánh giá của các mô hình mã nguồn mở, các tác giả đề xuất ProJudge-173k, một tập dữ liệu lớn cho việc tinh chỉnh hướng dẫn, cùng với một chiến lược tinh chỉnh hai giai đoạn động (Dynamic Dual-Phase - DDP). Phương pháp này khuyến khích các mô hình suy luận qua các vấn đề trước khi đánh giá các giải pháp, từ đó cải thiện đáng kể hiệu suất của chúng.

Nghiên cứu cho thấy có một khoảng cách về hiệu suất giữa các mô hình độc quyền và mã nguồn mở, với việc tinh chỉnh trên ProJudge-173k giúp thu hẹp khoảng cách này. Kết quả chỉ ra rằng các mô hình lớn hơn thường hoạt động tốt hơn, nhưng việc tinh chỉnh có thể giúp các mô hình nhỏ hơn đạt được kết quả tương đương. Bài báo kết luận bằng cách nhấn mạnh tầm quan trọng của việc đánh giá quy trình trong việc hiểu rõ điểm yếu của mô hình và hướng dẫn các cải tiến trong tương lai đối với khả năng suy luận đa phương thức.
